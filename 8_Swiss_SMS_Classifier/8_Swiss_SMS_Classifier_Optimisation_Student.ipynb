{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swiss SMS Classifier Optimisation\n",
    "\n",
    "Optimisation of real-world AI applications is a difficult process that requires a structured aproach.\n",
    "\n",
    "For ready-made, out-of-the-box machine learning applications such as MNIST or the IRIS classification task the term optimisation is usually focused on algorithm choice, and hyperparameter tuning.\n",
    "\n",
    "For real-world use cases where one starts from scratch optimisation has to take also the steps of training collection creation & management, and evaluation (especially in the sense of validity) into consideration.\n",
    "\n",
    "As described in the slides \"Gears of Machine Learning\" this means looking at the following three areas with regard to improvement:\n",
    "\n",
    "* Training Data Curation\n",
    "* Ml Algorithmic Optimisations\n",
    "* Evaluation (Validity)\n",
    "\n",
    "Specifically the last point is one that is often neglected, but absolutely crucial in terms of production-grade level success of AI/ML solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Exercise 1: Algorithmic Optimisation`\n",
    "\n",
    "Algorithmic optimisation is often the first point people turn to when trying to increase observered performance.\n",
    "\n",
    "As a first exercise try to increase the observed performance for the SMS classifier for content_type.\n",
    "\n",
    "1. Try to have a structured approach where you define beforehand what scenarios you want to test (e.g. which different algorithms you want to test).\n",
    "2. Consider the certainty of your observations. How sure are we that the ranking we observe for our classifiers is valid? If we base our choice for the production system on a classifier based on scores; how high is our confidence?\n",
    "\n",
    "Report your observations to the class on the following sheet:\n",
    "\n",
    "https://docs.google.com/spreadsheets/d/1zWnt6mwQY9KoWFXZpSQ6rsdOn56YL7-PYVdvu-_IwDo/edit?usp=sharing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Exercise 2: Training Set Curation`\n",
    "\n",
    "Re-visiting the training set is a second possibility in terms of optimisation. In the real world this happens very often and frequently during the lifetime of a machine learning project (Machine trainer is a professional term that has gained traction as of late).\n",
    "\n",
    "Some practical considerations when revisiting the trainining material:\n",
    "* Estimate necessity for multiple assessors: Not having 2+ assessors go over the same samples can speed up the creation of labelled samples. It is an optimisation that is often initially applied partially. E.g. by having only a selected sub-set of the labelled samples be annotated by multiple people, and the majority by only 1 person.\n",
    "* Management of iteratively created sets. Each new addition of the training set is usually created under different circumstances. It is therefore good practice to test performance also on the subsets themselves. This can be very useful in terms of identifying mistakes in the curation, but also is very valuable input for the validity optimisation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Exercise 3: Evaluation & Validity`\n",
    "\n",
    "Evaluation & Validity is often the hardest and most crucial part of AI application development. \n",
    "\n",
    "Validity itself is a very hard topic to approach: How do we know if a measurement instrument is accurate and measuring what it is supposed to measure? Think about a Thermometer; how do you know that it is accurate?\n",
    "\n",
    "There are a couple of main approaches towards this:\n",
    "\n",
    "* Calibration approach: Measuring with multiple sets\n",
    "* Fine-grained analysis: Evaluating the indidivudal label scores. How close are the scores between some labels? How distinctive is the scoring? This can also go to the level of analysing the learned weights (a.k.a parameters) of a model. What is the model really classifying?\n",
    "* Qualitative Assessments: Conduct further tests with synthetic input to assess the behaviour of the classifier.\n",
    "\n",
    "Some technical approaches allow us to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Exercise 3a: Calibration`\n",
    "\n",
    "Make use of the additional sets created under Exercise 2 as calibration tools. \n",
    "\n",
    "One way to do this is to rank multiple classifiers based on these sets. \n",
    "The intuition being, that high overlap in these ranking indicates that they are measuring the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Exercise 3b: Drill Down Analysis`\n",
    "\n",
    "Drill down analysis means to have a look at the score distribution:\n",
    "* How close are the scores?\n",
    "* Do they make sense in terms of the task?\n",
    "\n",
    "Use the tooling that sci-kit classifiers bring to look at the scores per sample.\n",
    "\n",
    "Drill down analysis can also mean to inspect and have a look at the parameters. sci-kit classifiers have varying support for this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Exercise 3c: Qualitative Assessment & Semi-Supervision`\n",
    "\n",
    "Lastly, nothing beats taking a close manual look with unseen examples. \n",
    "This can often be combined with the drill-down analysis to automate part of this process. \n",
    "\n",
    "We can base our assessment on samples where the classifier had very high confidence in order to assess if we are really training for the correct classes.\n",
    "\n",
    "* Make use of the per sample scores in order to create subsets of high confidence samples and subsets of low confidence samples. Use those to do the manual analysis process. This approach is one example of semi-supervision. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Round II of Algorithm Optimisation\n",
    "\n",
    "Have another run of algorithm optmisations. Report the new observations on:\n",
    "\n",
    "https://docs.google.com/spreadsheets/d/1zWnt6mwQY9KoWFXZpSQ6rsdOn56YL7-PYVdvu-_IwDo/edit?usp=sharing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
